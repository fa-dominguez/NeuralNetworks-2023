{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e18072",
   "metadata": {},
   "source": [
    "<div style=\"float: right; margin: 0px 15px 15px 0px;\">x\n",
    "</div>\n",
    "<h1> Assignment 2 - Single Neuron Classification </h1>\n",
    "<em> <strong>Deep Learning - MITxPro </strong></em>\n",
    "<br><br>\n",
    "Written by Felipe Dominguez<br>\n",
    "03/05/23 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3959aee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "      <th>color</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.029</td>\n",
       "      <td>18.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>0.99230</td>\n",
       "      <td>3.23</td>\n",
       "      <td>0.52</td>\n",
       "      <td>11.8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.38</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.032</td>\n",
       "      <td>29.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>0.99298</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.54</td>\n",
       "      <td>9.7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.7</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.41</td>\n",
       "      <td>9.4</td>\n",
       "      <td>0.040</td>\n",
       "      <td>49.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>0.99540</td>\n",
       "      <td>3.12</td>\n",
       "      <td>0.61</td>\n",
       "      <td>9.9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.29</td>\n",
       "      <td>15.5</td>\n",
       "      <td>0.064</td>\n",
       "      <td>56.0</td>\n",
       "      <td>115.5</td>\n",
       "      <td>0.99737</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.41</td>\n",
       "      <td>9.7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.7</td>\n",
       "      <td>0.060</td>\n",
       "      <td>19.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.31</td>\n",
       "      <td>10.4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0            8.1              0.28         0.39             1.9      0.029   \n",
       "1            6.5              0.23         0.38             1.3      0.032   \n",
       "2            6.7              0.24         0.41             9.4      0.040   \n",
       "3            7.1              0.13         0.29            15.5      0.064   \n",
       "4            5.5              0.24         0.32             8.7      0.060   \n",
       "\n",
       "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                 18.0                  79.0  0.99230  3.23       0.52   \n",
       "1                 29.0                 112.0  0.99298  3.29       0.54   \n",
       "2                 49.0                 166.0  0.99540  3.12       0.61   \n",
       "3                 56.0                 115.5  0.99737  3.16       0.41   \n",
       "4                 19.0                 102.0  0.99400  3.27       0.31   \n",
       "\n",
       "   alcohol  quality  color  \n",
       "0     11.8        6      0  \n",
       "1      9.7        5      0  \n",
       "2      9.9        6      0  \n",
       "3      9.7        7      0  \n",
       "4     10.4        5      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# These are the packages required for this assignment\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Use Pandas to read the csv file into a dataframe.\n",
    "# Note that the delimiter in this csv is the semicolon \";\" instead of a ,\n",
    "df_red = pd.read_csv('winequality-red.csv',delimiter=\";\")\n",
    "\n",
    "# Because we are performing a classification task, we will assign all red wine a label of 1\n",
    "df_red[\"color\"] = 1\n",
    "\n",
    "# The method .head() is super useful for seeing a preview of our data!\n",
    "df_red.head()\n",
    "\n",
    "df_white = pd.read_csv('winequality-white.csv',delimiter=\";\")\n",
    "df_white[\"color\"] = 0  #assign white wine the label 0\n",
    "df_white.head()\n",
    "\n",
    "# Now we combine our two dataframes\n",
    "df = pd.concat([df_red, df_white])\n",
    "\n",
    "# And shuffle them in place to mix the red and white wine data together\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748e63c8",
   "metadata": {},
   "source": [
    "The following block of code create the class object for a Single Neuron Model as well as it's child for a Classification problem.\n",
    "In this code it is important to note that:\n",
    "\n",
    "<ul> \n",
    "    <li> Weights are initialized randomly to avoid deterministic</li>\n",
    "    <li> The activation function is a Sigmoid function. The algorithm is evaluated by the following structure</li>\n",
    "        <ul>\n",
    "            <li>If activate function greater than 0.5 -> Red wine</li>\n",
    "            <li>If activate function lower or equal than 0.5 -> White wine</li>\n",
    "        </ul>\n",
    "    <li>Use of the Negative log likelihood as loss function</li>\n",
    "</ul>\n",
    "\n",
    "The Single Neuron Classification Model had an accuracy of ~<b>92%</b> with a total of 5982 correct prediction out of 6497."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a785b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the single neuron model\n",
    "class SingleNeuronModel():\n",
    "    \n",
    "    # Initiate class object and create random weights\n",
    "    def __init__(self, in_features):\n",
    "        self.w = 0.01 * np.random.rand(in_features)\n",
    "        self.w_0 = 0.01 * np.random.randn()\n",
    "        self.non_zero_tolerance = 1e-18\n",
    "    \n",
    "    # function to predict the value of 'x'\n",
    "    def forward(self, x):\n",
    "        \n",
    "        if (x.shape != self.w.shape):\n",
    "            raise ValueError('Shape of input x and weights w should be the same')\n",
    "        \n",
    "        # dot product between x and w transpose + w_0\n",
    "        self.z = x @ self.w.T + self.w_0\n",
    "        \n",
    "        # Calling the activation function\n",
    "        self.activate_f = self.activation(self.z)\n",
    "        \n",
    "        return self.activate_f\n",
    "\n",
    "    \n",
    "    def activation(self, z):\n",
    "        raise ImplementationError(\"activation method should be implemented by subclass\")\n",
    "    \n",
    "    # calculate and save gradient of our output with respect to weights\n",
    "    def gradient(self, x):\n",
    "        raise ImplementationError(\"gradient method should be implemented by subclass\")\n",
    "    \n",
    "    # Update the weights based on the gradient descending algorithm\n",
    "    def update(self, grad_loss, learning_rate):\n",
    "        model.w   -= grad_loss * self.grad_w   * learning_rate\n",
    "        model.w_0 -= grad_loss * self.grad_w_0 * learning_rate\n",
    "        \n",
    "class SingleNeuronClassificationModel(SingleNeuronModel):\n",
    "\n",
    "    # sigmoid function for classification problems\n",
    "    def activation(self, z):\n",
    "\n",
    "        sigmoid_f = 1 / (1 + np.exp(-z) +self.non_zero_tolerance)\n",
    "\n",
    "        return sigmoid_f\n",
    "    \n",
    "    def gradient(self, x):\n",
    "        self.grad_w = self.activate_f * (1 - self.activate_f) * x\n",
    "        self.grad_w_0 = self.activate_f * (1 - self.activate_f)\n",
    "\n",
    "def train_model_NLL_loss(model, i_data, o_data, learning_rate, num_epochs):\n",
    "    \n",
    "    non_zero_tolerance = 1e-18\n",
    "    num_samples = len(i_data)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        total_loss = 0 \n",
    "        \n",
    "        for i in range(num_samples):\n",
    "            x = i_data[i, ...]\n",
    "            y = o_data[i]\n",
    "            y_predicted = model.forward(x)\n",
    "            \n",
    "            # negative log likelihood function\n",
    "            nll_loss = -(y * np.log(y_predicted + non_zero_tolerance) + (1-y) * \\\n",
    "                         np.log(1-y_predicted + non_zero_tolerance))\n",
    "            \n",
    "            total_loss += nll_loss\n",
    "            \n",
    "            # Calculate gradient\n",
    "            model.gradient(x)\n",
    "            \n",
    "            # Calculate gradient loss\n",
    "            grad_loss = (y_predicted - y)/(y_predicted * (1-y_predicted) + non_zero_tolerance)\n",
    "            \n",
    "            # Update weights\n",
    "            model.update(grad_loss, learning_rate)\n",
    "        \n",
    "        report_every = max(1, num_epochs // 10)\n",
    "        if epoch == 1 or epoch % report_every == 0: #every few epochs, report\n",
    "            print(\"epoch\", epoch, \"has total loss\", total_loss)\n",
    "\n",
    "\n",
    "# We will use this function to evaluate how well our trained classifier perfom\n",
    "# Hint: the model you define above must have a .forward function in order to be compatible\n",
    "# Hint: this evaluation function is identical to those in previous notebooks\n",
    "def evaluate_classification_accuracy(model, input_data, labels):\n",
    "    # Count the number of correctly classified samples given a set of weights\n",
    "    correct = 0\n",
    "    num_samples = len(input_data)\n",
    "    for i in range(num_samples):\n",
    "        x = input_data[i,...]\n",
    "        y = labels[i]\n",
    "        y_predicted = model.forward(x)\n",
    "        label_predicted = 1 if y_predicted > 0.5 else 0\n",
    "        if label_predicted == y:\n",
    "            correct += 1\n",
    "    accuracy = correct / num_samples\n",
    "    print(\"Our model predicted\", correct, \"out of\", num_samples,\n",
    "          \"correctly for\", accuracy*100, \"% accuracy\")\n",
    "    return accuracy\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da7a1568",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (6497, 3)\n",
      "Shape of Y: (6497, 1)\n"
     ]
    }
   ],
   "source": [
    "# We choose three attributes of the wine to perform our prediction on\n",
    "input_columns = [\"citric acid\", \"residual sugar\", \"total sulfur dioxide\"]\n",
    "output_columns = [\"color\"]\n",
    "\n",
    "# We extract the relevant features into our X and Y numpy arrays\n",
    "X = df[input_columns].to_numpy()\n",
    "Y = df[output_columns].to_numpy()\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n",
    "in_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd1ab035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 has total loss [6881.09277825]\n",
      "epoch 20 has total loss [3704.82842878]\n",
      "epoch 40 has total loss [3487.75931964]\n",
      "epoch 60 has total loss [3452.36536465]\n",
      "epoch 80 has total loss [3435.1420106]\n",
      "epoch 100 has total loss [3428.82154786]\n",
      "epoch 120 has total loss [3431.8407891]\n",
      "epoch 140 has total loss [3435.0269211]\n",
      "epoch 160 has total loss [3433.12928786]\n",
      "epoch 180 has total loss [3436.38279803]\n",
      "epoch 200 has total loss [3435.79812388]\n",
      "\n",
      "Final weights:\n",
      "[-0.38548648 -0.43523312 -0.18036022] [12.03483559]\n",
      "Our model predicted 5983 out of 6497 correctly for 92.08865630290903 % accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9208865630290903"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model & run the model\n",
    "learning_rate = 0.001\n",
    "epochs = 200\n",
    "\n",
    "model = SingleNeuronClassificationModel(in_features=len(X[1]))\n",
    "train_model_NLL_loss(model, X, Y, learning_rate, epochs)\n",
    "print(\"\\nFinal weights:\")\n",
    "print(model.w, model.w_0)\n",
    "evaluate_classification_accuracy(model, X, Y)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
